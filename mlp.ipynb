{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation\n",
    " - nb = 0 => extraire info\n",
    " - 1ere substance sortie\n",
    " - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'substances'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-8d4f59e4071f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'substances'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'substances'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'substances'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mD:\\tools\\Anaconda2\\Lib\\site-packages\\pandas\\core\\series.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\tools\\Anaconda2\\Lib\\site-packages\\pandas\\indexes\\base.pyc\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   1978\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1979\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 1980\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   1981\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1982\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'integer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'boolean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_value (pandas\\index.c:3332)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_value (pandas\\index.c:3035)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4084)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'substances'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor, BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.preprocessing import normalize, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "data_dir = 'D:/Dev/dataScience/kaggle/challenge_25_data/data/'\n",
    "\n",
    "# features numériques\n",
    "feat_num = ['libelle_plaquette', 'libelle_ampoule', 'libelle_flacon', 'libelle_tube', 'libelle_stylo', 'libelle_seringue',\n",
    "            'libelle_pilulier', 'libelle_sachet', 'libelle_comprime', 'libelle_gelule', 'libelle_film', 'libelle_poche',\n",
    "            'libelle_capsule'] + ['nb_plaquette', 'nb_ampoule', 'nb_flacon', 'nb_tube', 'nb_stylo', 'nb_seringue',\n",
    "            'nb_pilulier', 'nb_sachet', 'nb_comprime', 'nb_gelule', 'nb_film', 'nb_poche', 'nb_capsule', 'nb_ml']\n",
    "# features date\n",
    "feat_dates = ['date declar annee', 'date amm annee']\n",
    "# features catégorielles\n",
    "feat_cat = ['statut', 'etat commerc', 'agrement col', 'tx rembours', 'voies admin', 'statut admin', 'type proc']\n",
    "# features texte\n",
    "feat_text = ['libelle', 'titulaires', 'substances', 'forme pharma']\n",
    "\n",
    "def mape_error(log_y_true, log_y_pred): \n",
    "    y_true = np.exp(log_y_true)\n",
    "    y_pred = np.exp(log_y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "mape_scorer = make_scorer(mape_error, greater_is_better = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import des données\n",
    " - ```train``` : 8564 medicaments / 41 variables\n",
    " - ```test``` : 3671 medicaments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 43\n",
    "np.random.seed(seed)\n",
    "train = pd.read_csv(data_dir + 'boites_medicaments_train.csv',encoding='utf-8',sep=';')\n",
    "test = pd.read_csv(data_dir + 'boites_medicaments_test.csv',encoding='utf-8',sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation des donnees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodage des features catégorielles\n",
    "\n",
    "Les algorithmes de machine learning s'attendent à avoir en entrée des nombres, et non pas des chaînes de caractères. C'est pourquoi nous transformons les features catégorielles en nombres, à l'aide de LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in feat_cat:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train[c].append(test[c]))\n",
    "    train[c] = le.transform(train[c])\n",
    "    test[c] = le.transform(test[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Split des catégories multi-valeurs\n",
    " Les catégories dont les valeurs sont des listes d'éléments sont développées sous forme de n catégories binaires (n étant le nombre d'éléments distincts pour la catégorie dans le dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def expandedOHE(train_df, test_df, colName):\n",
    "    # type: (DataFrame, DataFrame, str) -> *str\n",
    "    distinctCategs = (train_df[colName]\n",
    "                      .apply(lambda st : st.split(','))\n",
    "                      .apply(pd.Series)\n",
    "                      .unstack()\n",
    "                      .dropna()\n",
    "                      .str.strip()\n",
    "                      .unique())\n",
    "    for categorie in distinctCategs:\n",
    "        train_df[categorie] = train_df[colName].apply(lambda x : 1 if categorie in x else 0)\n",
    "        test_df[categorie] = test_df[colName].apply(lambda x : 1 if categorie in x else 0)\n",
    "    return list(distinctCategs) \n",
    "\n",
    "## le split de la categorie \"substances\" permet de passer de 45 à 32%\n",
    "feat_substances = expandedOHE(train, test, 'substances') \n",
    "## le split de la categorie \"voies admin\" dégrade l'estimation\n",
    "# feat_substances = expandedOHE(train, test, 'substances') \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantites et prix unitaire (prix/quantite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['logprix'] = train['prix'].apply(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['cst1'] = 1\n",
    "test['cst1'] = 1\n",
    "train['nb'] = (train[['nb_plaquette', 'nb_ampoule', 'nb_flacon', 'nb_tube', 'nb_ampoule', 'nb_flacon', 'nb_tube', \n",
    "              'nb_stylo', 'nb_seringue', 'nb_pilulier', 'nb_sachet', 'nb_film', 'nb_poche', 'cst1']].max(axis=1) \n",
    "              * train[['nb_comprime', 'nb_gelule', 'nb_capsule', 'nb_ml', 'cst1']].max(axis=1))\n",
    "test['nb'] = (test[['nb_plaquette', 'nb_ampoule', 'nb_flacon', 'nb_tube', 'nb_ampoule', 'nb_flacon', 'nb_tube', \n",
    "              'nb_stylo', 'nb_seringue', 'nb_pilulier', 'nb_sachet', 'nb_film', 'nb_poche', 'cst1']].max(axis=1) \n",
    "              * test[['nb_comprime', 'nb_gelule', 'nb_capsule', 'nb_ml', 'cst1']].max(axis=1))\n",
    "\n",
    "# Visualisation distribution 'logprixunit'\n",
    "train['prixunit'] = train['nb'] * train['prix']\n",
    "train['logprixunit'] = train['prixunit'].apply(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enregistrement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv(data_dir + 'train_df.csv', encoding='utf-8', sep=';')\n",
    "test.to_csv(data_dir + 'test_df.csv', encoding='utf-8', sep=';')\n",
    "\n",
    "with open(data_dir + 'substances.pkl', 'wb') as f:\n",
    "    pickle.dump(feat_substances, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation d'un modele"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation \n",
    "(imports + chargement des datasets préparés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor, BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.preprocessing import normalize, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "data_dir = 'D:/Dev/dataScience/kaggle/challenge_25_data/data/'\n",
    "\n",
    "# features numériques\n",
    "feat_num = ['libelle_plaquette', 'libelle_ampoule', 'libelle_flacon', 'libelle_tube', 'libelle_stylo', 'libelle_seringue',\n",
    "            'libelle_pilulier', 'libelle_sachet', 'libelle_comprime', 'libelle_gelule', 'libelle_film', 'libelle_poche',\n",
    "            'libelle_capsule'] + ['nb_plaquette', 'nb_ampoule', 'nb_flacon', 'nb_tube', 'nb_stylo', 'nb_seringue',\n",
    "            'nb_pilulier', 'nb_sachet', 'nb_comprime', 'nb_gelule', 'nb_film', 'nb_poche', 'nb_capsule', 'nb_ml']\n",
    "# features date\n",
    "feat_dates = ['date declar annee', 'date amm annee']\n",
    "# features catégorielles\n",
    "feat_cat = ['statut', 'etat commerc', 'agrement col', 'tx rembours', 'voies admin', 'statut admin', 'type proc']\n",
    "# features texte\n",
    "feat_text = ['libelle', 'titulaires', 'substances', 'forme pharma']\n",
    "\n",
    "def mape_error(log_y_true, log_y_pred): \n",
    "    y_true = np.exp(log_y_true)\n",
    "    y_pred = np.exp(log_y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def grid_search_mape(estimator, X, Y, parameters, nb_folds):\n",
    "    print(\"Performing grid search...\")\n",
    "    grid_search = GridSearchCV(estimator, parameters, scoring=mape_scorer, cv=nb_folds)\n",
    "    \n",
    "    print \"pipeline:   \"  + str([name for name, _ in pipeline.steps])\n",
    "    t0 = time()\n",
    "    grid_search.fit(X, Y)\n",
    "    print \"=> done in %0.3fs\" % (time() - t0)\n",
    "    print \"Best score: %0.3f\" % grid_search.best_score_\n",
    "    print \"Best parameters set:\"\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    res = grid_search.cv_results_\n",
    "    print \"Results by parameters:\"\n",
    "    for i in range(0, len(res.get('params'))):\n",
    "        p = res.get('params')[i]\n",
    "        m = res.get('mean_test_score')[i]\n",
    "        s = res.get('std_test_score')[i]\n",
    "        print p\n",
    "        print '\\tMape: %.2f\\t(std: %.2f)' % (m,s)\n",
    "    return grid_search\n",
    "\n",
    "mape_scorer = make_scorer(mape_error, greater_is_better = False)\n",
    "\n",
    "# chargement des datasets préparés\n",
    "train = pd.read_csv(data_dir + 'train_df.csv',encoding='utf-8',sep=';')\n",
    "test = pd.read_csv(data_dir + 'test_df.csv',encoding='utf-8',sep=';')\n",
    "\n",
    "with open(data_dir + 'substances.pkl', 'rb') as f:\n",
    "    feat_substances = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feats = feat_num + feat_cat + feat_substances ## chug on ajoute les feat_substances\n",
    "\n",
    "Y = train['logprix']\n",
    "X = train[feats]\n",
    "\n",
    "#GradientBoostingRegressor, ExtraTreesRegressor, BaggingRegressor, RandomForestRegressor\n",
    "reg = RandomForestRegressor(n_jobs=4) \n",
    "pipeline = make_pipeline(StandardScaler(), reg) #, \n",
    "\n",
    "parameters = {'randomforestregressor__n_estimators':  [10, 20, 50, 100], \n",
    "              'randomforestregressor__min_impurity_split': [1e-5, 1e-6, 1e-7, 1e-8]}\n",
    "                #,'randomforestregressor__max_features': ['auto', 'sqrt']}\n",
    "\n",
    "grid_search_mape(pipeline, X, Y, parameters, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split into input (X) and output (Y) variables\n",
    "FEATURES = feat_num + feat_cat + feat_substances ## chug on ajoute les feat_substances\n",
    "\n",
    "# create model\n",
    "# hyperopt / bash normalization / dropout / normalization entre les couches / 800 epoch / batch 50 / early stopping \n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=len(FEATURES), init='normal', activation='relu'))\n",
    "    #model.add(Dense(15, init='normal', activation='relu'))\n",
    "    #model.add(Dense(6, init='normal', activation='sigmoid'))\n",
    "    model.add(Dense(1, init='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mape', optimizer='rmsprop')\n",
    "    return model\n",
    "\n",
    "X = train[FEATURES]\n",
    "Y = train['logprix']\n",
    "\n",
    "reg = KerasRegressor(build_fn=baseline_model, nb_epoch=3, verbose=0)\n",
    "pipeline = make_pipeline(StandardScaler(), reg)\n",
    "\n",
    "parameters = {'kerasregressor__nb_epoch':  [2], 'kerasregressor__batch_size': [20,50]}\n",
    "grid_search = grid_search_mape(pipeline, X, Y, parameters, 2)\n",
    "\n",
    "'''\n",
    "kfold = KFold(n_splits=5, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold, scoring=mape_scorer, verbose=1)\n",
    "print (\"\\nResults: %.2f (%.2f) MAPE\" % (results.mean(), results.std()))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions et soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# configure regressor with best params from grid_search\n",
    "reg = ExtraTreesRegressor(n_estimators=20, n_jobs=4)\n",
    "pipeline = make_pipeline(StandardScaler(), reg)\n",
    "\n",
    "# fit on full train dataset\n",
    "pipeline.fit(train[feats], train['logprix'])\n",
    "\n",
    "# predicttest prices\n",
    "predictions = np.exp(pipeline.predict(test[feats]))\n",
    "\n",
    "# write to soumission.csv\n",
    "pd.DataFrame(predictions, index=test['id']).to_csv(data_dir + 'soumission.csv',  \n",
    "                          header=['prix'],\n",
    "                          sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimations de base : 65%\n",
    "\n",
    "Optimisations :\n",
    "- Ajout catégorie \"substances\" : 45%\n",
    "- Ajout catégorie \"substances splitées\" : \n",
    "    - FR : 32,5%\n",
    "    - ET : 33,9%\n",
    "    - GB : 69,6%\n",
    "    - AB : 129%\n",
    "    - BR : 35,0 %\n",
    "    \n",
    "\n",
    "Dégradations :\n",
    "- logprixnuit : +7/8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
