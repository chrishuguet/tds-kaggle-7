{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor, BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.preprocessing import normalize, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_DIR = '/home/memer/dev/ml/kaggle7/data/'\n",
    "\n",
    "# features numériques\n",
    "FEAT_NUM_IN = ['libelle_plaquette', 'libelle_ampoule', 'libelle_flacon', 'libelle_tube', 'libelle_stylo', 'libelle_seringue',\n",
    "            'libelle_pilulier', 'libelle_sachet', 'libelle_comprime', 'libelle_gelule', 'libelle_film', 'libelle_poche',\n",
    "            'libelle_capsule'] + ['nb_plaquette', 'nb_ampoule', 'nb_flacon', 'nb_tube', 'nb_stylo', 'nb_seringue',\n",
    "            'nb_pilulier', 'nb_sachet', 'nb_comprime', 'nb_gelule', 'nb_film', 'nb_poche', 'nb_capsule', 'nb_ml']\n",
    "# features date\n",
    "FEAT_DATE_IN = ['date declar annee', 'date amm annee']\n",
    "# features catégorielles\n",
    "FEAT_CAT_IN = ['statut', 'etat commerc', 'agrement col', 'tx rembours', 'statut admin', 'type proc']\n",
    "# features texte\n",
    "FEAT_TEXT_IN = ['libelle', 'substances', 'voies admin', 'titulaires', 'forme pharma']\n",
    "\n",
    "FEAT_ALL_IN = FEAT_NUM_IN + FEAT_DATE_IN + FEAT_CAT_IN + FEAT_TEXT_IN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def mape_error(log_y_true, log_y_pred): \n",
    "    # type: (Series, Series) -> Series\n",
    "    y_true = np.exp(log_y_true)\n",
    "    y_pred = np.exp(log_y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "mape_scorer = make_scorer(mape_error, greater_is_better = False)\n",
    "\n",
    "def grid_search_mape(estimator, X, Y, parameters, nb_folds):\n",
    "    print(\"Performing grid search...\")\n",
    "    grid_search = GridSearchCV(estimator, parameters, scoring=mape_scorer, cv=nb_folds, n_jobs=-1)\n",
    "    \n",
    "    print \"pipeline:   \"  + str([name for name, _ in pipeline.steps])\n",
    "    t0 = time.time()\n",
    "    grid_search.fit(X, Y)\n",
    "    print \"=> done in %0.3fs\" % (time.time() - t0)\n",
    "    print \"Best score: %0.3f\" % grid_search.best_score_\n",
    "    print \"Best parameters set:\"\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    res = grid_search.cv_results_\n",
    "    print \"Results by parameters:\"\n",
    "    for i in range(0, len(res.get('params'))):\n",
    "        p = res.get('params')[i]\n",
    "        m = res.get('mean_test_score')[i]\n",
    "        s = res.get('std_test_score')[i]\n",
    "        print p\n",
    "        print '\\tMape: %.2f\\t(std: %.2f)' % (m,s)\n",
    "\n",
    "def displayUniqueCount(series):\n",
    "    # type: (Series) -> DataFrame\n",
    "    data = np.unique(series, return_counts=True)\n",
    "    df = pd.DataFrame(\n",
    "        data = {'Valeur':data[0], 'Nombre':data[1]},\n",
    "        columns = ['Valeur', 'Nombre'])\n",
    "    df.sort_values(by =\"Nombre\", ascending=False, inplace=True)\n",
    "    return df\n",
    "\n",
    "def agg_duplicate_rows(df, feats_gb, feat_target, agg_func):\n",
    "    # type: (DataFrame, *str, str, function) -> DataFrame\n",
    "    mean_target_df = df.groupby(feats_gb)[feat_target].agg({'mean_target' : agg_func})\n",
    "    result_df = (df\n",
    "                 .drop_duplicates(feats_gb)\n",
    "                 .set_index(feats_gb)\n",
    "                 .join(mean_target_df)\n",
    "                 .reset_index(level=feats_gb)\n",
    "                 .drop(feat_target, axis=1)\n",
    "                 .rename(columns={'mean_target': feat_target})\n",
    "                )\n",
    "    return result_df\n",
    "\n",
    "def addNumColsFromLibelle(df, libelles_to_extract):\n",
    "    # type: (DataFrame, str) -> *str\n",
    "    new_feats_col = []\n",
    "    \n",
    "    for lib in libelles_to_extract:\n",
    "        nb_col_name = 'nb_' + lib\n",
    "        lib_col_name = 'libelle_' + lib\n",
    "        pattern = r'([0-9]*) ?' + lib\n",
    "        \n",
    "        df[nb_col_name] = (df['libelle']\n",
    "                                .str.extract(pattern, expand=False) # extract pattern \n",
    "                                .str.replace(r'^$', '1')            # replace match with empty group by 1\n",
    "                                .fillna(0)                          # replace mismatch by 0\n",
    "                                .astype(int)                        # convert string to int\n",
    "                                )\n",
    "        df[lib_col_name] = df.libelle.apply(lambda x: 1 if lib in x else 0)\n",
    "\n",
    "        new_feats_col.append(nb_col_name)\n",
    "        new_feats_col.append(lib_col_name)\n",
    "        \n",
    "    return new_feats_col\n",
    "\n",
    "def kfold_validation(X, Y, regressor):\n",
    "        error_global = 0\n",
    "        NBROUND = 5\n",
    "        kf = KFold(n_splits=NBROUND, shuffle=True, random_state=SEED)\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            start = time.time()\n",
    "            X_train, X_test = X.ix[train_index, :], X.ix[test_index, :]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "            \n",
    "            regressor.fit(X_train, Y_train)\n",
    "            pred = regressor.predict(X_test)\n",
    "            \n",
    "            error_fold = mape_error(Y_test, pred)\n",
    "            error_global += error_fold / NBROUND\n",
    "            print str(error_fold) + ' duration: ' + str(time.time() - start)\n",
    "            \n",
    "        print 'MAPE Error : ' + str(error_global)\n",
    "        return error_global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import des données\n",
    " - ```train``` : 8564 medicaments / 41 variables\n",
    " - ```test``` : 3671 medicaments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_DIR + 'boites_medicaments_train.csv',encoding='utf-8',sep=';')\n",
    "test = pd.read_csv(DATA_DIR + 'boites_medicaments_test.csv',encoding='utf-8',sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation des donnees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction de libellés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "libelles_to_extract = ['cartouche', 'bouteille', 'inhalateur', 'multidose']\n",
    "feat_num_new = addNumColsFromLibelle(train, libelles_to_extract)\n",
    "addNumColsFromLibelle(test, libelles_to_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feats_groupby = FEAT_ALL_IN + feat_num_new\n",
    "feat_target = 'prix'\n",
    "\n",
    "train = agg_duplicate_rows(train, feats_groupby, feat_target, np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodage des features catégorielles\n",
    "\n",
    "Les algorithmes de machine learning s'attendent à avoir en entrée des nombres, et non pas des chaînes de caractères. C'est pourquoi nous transformons les features catégorielles en nombres, à l'aide de LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in FEAT_CAT_IN:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train[c].append(test[c]))\n",
    "    train[c] = le.transform(train[c])\n",
    "    test[c] = le.transform(test[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Split des catégories multi-valeurs\n",
    " Les catégories dont les valeurs sont des listes d'éléments sont développées sous forme de n catégories binaires (n étant le nombre d'éléments distincts pour la catégorie dans le dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['substances_clean'] = (train['substances'].str.replace(r'\\([^\\)]*\\)', ''))\n",
    "test['substances_clean'] = (test['substances'].str.replace(r'\\([^\\)]*\\)', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def expandedOHE(train_df, test_df, colName):\n",
    "    # type: (DataFrame, DataFrame, str) -> *str\n",
    "    distinctCategs = (train_df[colName]\n",
    "                      .apply(lambda st : st.split(','))\n",
    "                      .apply(pd.Series)\n",
    "                      .unstack()\n",
    "                      .dropna()\n",
    "                      .str.strip()\n",
    "                      .unique())\n",
    "    for categorie in distinctCategs:\n",
    "        train_df[categorie] = train_df[colName].apply(lambda x : 1 if categorie in x else 0)\n",
    "        test_df[categorie] = test_df[colName].apply(lambda x : 1 if categorie in x else 0)\n",
    "    return list(distinctCategs) \n",
    "\n",
    "## le split de la categorie \"substances\" permet de passer de 45 à 32%\n",
    "feat_substances = expandedOHE(train, test, 'substances_clean')\n",
    "                            \n",
    "## le split de la categorie \"voies admin\" dégrade l'estimation\n",
    "feat_substances = expandedOHE(train, test, 'substances') \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Quantites et prix unitaire (prix/quantite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['logprix'] = train['prix'].apply(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation d'un modele"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressor : Grid search with cross val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline:   ['standardscaler', 'randomforestregressor']\n",
      "=> done in 24.320s\n",
      "Best score: -32.661\n",
      "Best parameters set:\n",
      "\trandomforestregressor__min_impurity_split: 1e-05\n",
      "\trandomforestregressor__n_estimators: 50\n",
      "Results by parameters:\n",
      "{'randomforestregressor__n_estimators': 50, 'randomforestregressor__min_impurity_split': 1e-05}\n",
      "\tMape: -32.66\t(std: 1.43)\n"
     ]
    }
   ],
   "source": [
    "feats = FEAT_NUM_IN + FEAT_CAT_IN + feat_substances + feat_num_new + feat_voies_admin\n",
    "\n",
    "Y = train['logprix']\n",
    "X = train[feats]\n",
    "\n",
    "#GradientBoostingRegressor, ExtraTreesRegressor, BaggingRegressor, RandomForestRegressor\n",
    "reg = RandomForestRegressor(n_jobs=-1, criterion='mse', random_state=SEED) \n",
    "pipeline = make_pipeline(StandardScaler(), reg) #, \n",
    "\n",
    "parameters = {'randomforestregressor__n_estimators':  [10, 50, 100], \n",
    "              'randomforestregressor__min_impurity_split': [1e-5, 1e-6]}\n",
    "\n",
    "grid_search_mape(pipeline, X, Y, parameters, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressor : Simple crossval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.2704234689 duration: 1.05395293236\n",
      "33.5823166283 duration: 0.926917076111\n",
      "33.813629656 duration: 1.06042194366\n",
      "31.1133842608 duration: 0.963861942291\n",
      "32.9382171978 duration: 1.01814508438\n",
      "MAPE Error : 32.5435942423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32.543594242348206"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = RandomForestRegressor(n_jobs=-1, random_state=SEED, n_estimators=50, min_impurity_split=1e-05) \n",
    "kfold_validation(X,Y,reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split into input (X) and output (Y) variables\n",
    "feats = FEAT_NUM_IN + FEAT_CAT_IN + feat_substances + feat_num_new\n",
    "\n",
    "# create model\n",
    "# hyperopt / bash normalization / dropout / normalization entre les couches / 800 epoch / batch 50 / early stopping \n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.2, input_shape=(len(feats),)))\n",
    "    model.add(Dense(100, input_dim=len(feats), init='normal', activation='relu', W_constraint=maxnorm(5)))\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(Dense(15, init='normal', activation='relu'))\n",
    "    model.add(Dense(50, init='normal', activation='sigmoid', W_constraint=maxnorm(5)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, init='normal', W_constraint=maxnorm(5)))\n",
    "    # Compile model\n",
    "    model.compile(loss='mape', optimizer='rmsprop')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search avec Kerasrgressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train[feats]\n",
    "Y = train['logprix']\n",
    "reg = KerasRegressor(build_fn=create_model, nb_epoch=10, batch_size=50, verbose=0)\n",
    "pipeline = make_pipeline(StandardScaler(), reg)\n",
    "\n",
    "parameters = {'kerasregressor__nb_epoch':  [10, 100, 1000], 'kerasregressor__batch_size': [50, 100]}\n",
    "grid_search = grid_search_mape(pipeline, X, Y, parameters, 5)\n",
    "\n",
    "'''\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold, scoring=mape_scorer, verbose=1)\n",
    "print (\"\\nResults: %.2f (%.2f) MAPE\" % (results.mean(), results.std()))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossval sans Kerasregressor\n",
    "Pour comparer les perfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "def nn_kfold_validation(X, Y, model_creator):\n",
    "        error_global = 0\n",
    "        NBROUND = 5\n",
    "        kf = KFold(n_splits=NBROUND, shuffle=True, random_state=SEED)\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            start = time.time()\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "            \n",
    "            model = None\n",
    "            model = model_creator()\n",
    "            model.fit(X_train, Y_train, nb_epoch=500, batch_size=50, verbose=0)\n",
    "            pred = model.predict(X_test)\n",
    "            \n",
    "            error_fold = mape_error(Y_test, pred[0])\n",
    "            error_global += error_fold / NBROUND\n",
    "            print str(error_fold) + ' duration: ' + str(time.time() - start)\n",
    "            \n",
    "        print 'MAPE Error : ' + str(error_global)\n",
    "        return error_global\n",
    "    \n",
    "X_scaler = StandardScaler()\n",
    "X = X_scaler.fit_transform(train[feats])\n",
    "Y = train['logprix']\n",
    "\n",
    "nn_kfold_validation(X, Y, create_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions et soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# configure regressor with best params from grid_search\n",
    "reg = RandomForestRegressor(n_jobs=-1, random_state=SEED, criterion='mae', n_estimators=50, min_impurity_split=1e-05) \n",
    "pipeline = make_pipeline(StandardScaler(), reg)\n",
    "# fit on full train dataset\n",
    "pipeline.fit(train[feats], train['logprix'])\n",
    "\n",
    "# predicttest prices\n",
    "predictions = np.exp(pipeline.predict(test[feats]))\n",
    "\n",
    "# write to soumission.csv\n",
    "pd.DataFrame(predictions, index=test['id']).to_csv(DATA_DIR + 'soumission.csv',  \n",
    "                          header=['prix'],\n",
    "                          sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enregistrement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.to_csv(DATA_DIR + 'train_df.csv', encoding='utf-8', sep=';')\n",
    "test.to_csv(DATA_DIR + 'test_df.csv', encoding='utf-8', sep=';')\n",
    "\n",
    "with open(DATA_DIR + 'substances.pkl', 'wb') as f:\n",
    "    pickle.dump(feat_substances, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# chargement des datasets préparés\n",
    "train = pd.read_csv(DATA_DIR + 'train_df.csv',encoding='utf-8',sep=';')\n",
    "test = pd.read_csv(DATA_DIR + 'test_df.csv',encoding='utf-8',sep=';')\n",
    "\n",
    "with open(DATA_DIR + 'substances.pkl', 'rb') as f:\n",
    "    feat_substances = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
